{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi.pyvi import ViTokenizer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from math import sqrt, pow, log\n",
    "import glob\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import io, re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(folderPath, folderSave):\n",
    "    files = glob.glob(folderPath)\n",
    "    documents = []\n",
    "    for f in files:\n",
    "        s = os.path.basename(f)\n",
    "        newpath = folderSave + s\n",
    "        doc = io.open(f, 'r')\n",
    "        clean = clean_words(doc.read().lower())\n",
    "#         doc = re.sub(r'[_]','', text.read())\n",
    "        newfile = io.open(newpath, 'w')\n",
    "        newfile.write(clean)\n",
    "#         documents.append(text.read())\n",
    "#     return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(folderPath):\n",
    "    files = glob.glob(folderPath)\n",
    "    files.sort()\n",
    "    documents = []\n",
    "    for f in files:\n",
    "        if files.index(f) == 45:\n",
    "            print files[45]\n",
    "#         print f\n",
    "        text = io.open(f, 'r')\n",
    "        documents.append(text.read())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(data):\n",
    "    wordscount = {}\n",
    "    for w in data.split():\n",
    "#         print w\n",
    "        if w in wordscount:\n",
    "            wordscount[w] += 1\n",
    "        else:\n",
    "            wordscount[w] = 1\n",
    "    return wordscount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_multi_doc(documents):\n",
    "    wordscount = {}\n",
    "    for doc in documents:\n",
    "        for w in doc.split():\n",
    "            if w in wordscount:\n",
    "                wordscount[w] += 1\n",
    "            else:\n",
    "                wordscount[w] = 1\n",
    "    return wordscount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(document):\n",
    "    count = word_count(document)\n",
    "    tf_metrics = {}\n",
    "    max_tf = float(find_tf_max(count))\n",
    "    for key in count:\n",
    "        a = float(count[key])\n",
    "        tf_metrics[key] = count[key] / max_tf\n",
    "    return tf_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tf_max(document):\n",
    "    max_tf = 0.0\n",
    "    for key in document:\n",
    "        if (document[key] > max_tf):\n",
    "            max_tf = document[key]\n",
    "    return max_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(dictionary, documents):\n",
    "    num_doc = len(documents)\n",
    "    idf_metric = {}\n",
    "    for word in dictionary:\n",
    "        count = 0\n",
    "        for doc in documents:\n",
    "            doc = doc.lower()\n",
    "            if word in doc.split():\n",
    "                count += 1\n",
    "#         print count\n",
    "        if (count == num_doc):\n",
    "            idf_metric[word] = log(num_doc/count)\n",
    "        else:\n",
    "            idf_metric[word] = log(num_doc/(1 + count))\n",
    "    return idf_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(dictionary, documents ):\n",
    "    # tf of word in a document\n",
    "    # idf of all word in documents (dictionary)\n",
    "    # create a matrix (words x number_of_documents)\n",
    "    num_words = len(dictionary)\n",
    "    num_doc = len(documents)\n",
    "    tfidf_matrix = np.zeros((num_doc, num_words))\n",
    "    idf = compute_idf(dictionary, documents)\n",
    "    \n",
    "    for row in range(num_doc):\n",
    "        tf = compute_tf(documents[row])\n",
    "        col = 0\n",
    "        for key in idf: \n",
    "            if key in tf:\n",
    "                tfidf_matrix[row][col] = tf[key]*idf[key]\n",
    "                col +=1\n",
    "            else:\n",
    "                tfidf_matrix[row][col] = 0\n",
    "                col +=1\n",
    "                \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(text):\n",
    "    word = re.sub(r'[.,;:-]|[\\d/\\n\\t]|[()\\\"\\\"\\'\\'\\*\\\\\\[\\]\\^\\=\\`]|[+!?@&#$%<>]|[_]+(?=[^a-z])',' ',text)\n",
    "    word = re.sub(r'( [a-z])+(?=[( )\\d])', '', word)\n",
    "    \n",
    "    word = word.lower()\n",
    "#     return word\n",
    "    return ViTokenizer.tokenize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(files):\n",
    "    data = list()\n",
    "    for x in files:\n",
    "        words = clean_words(x)\n",
    "        for w in words.split():\n",
    "            if w not in data:\n",
    "                data.append(w)\n",
    "    data.sort()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    path = '/home/dong/text_summarization/doc/output/doc/*.txt'\n",
    "    folder_save = '/home/dong/text_summarization/doc/output_edited/doc/*.txt'\n",
    "    dictionary = '/home/dong/text_summarization/doc/dictionary3.txt'\n",
    "#     filetext = read_file(path, folder_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dong/text_summarization/doc/output_edited/doc/24997.txt\n"
     ]
    }
   ],
   "source": [
    "files = load_file(folder_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = io.open(dictionary, 'r')\n",
    "dic = read.read()\n",
    "diction = []\n",
    "for x in dic.split():\n",
    "    diction.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_matrix(diction, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in files_clean:\n",
    "#     for word in doc.split():\n",
    "#         if word == u\"xe_gắn_máy\":\n",
    "#             print files_clean.index(doc)\n",
    "# #             print doc\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save = io.open(dictionary, 'w')\n",
    "# save.write('\\n'.join('%s' %x for x in dictFile))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
